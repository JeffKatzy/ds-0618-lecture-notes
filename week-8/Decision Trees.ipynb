{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Decision trees - are some of the most ubiquitous algorithms in data science \n",
    "\n",
    "* Decision trees were one of the first ML algorithms \n",
    "* And when ML was coming up as a field, were one of the first methodologies \n",
    "    * So we'll do this by tracing through a tree, and we take the chain of decisions that we make based on testing individual variables, and compare them to a single threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition the space \n",
    "\n",
    "* So think about this as taking the input space and partitioning it \n",
    "* So compare the datapoint to a threshold \n",
    "    So we divide up the input space into the different regions.  \n",
    "    \n",
    "* So then the entire region has a single value given by the hypothesis function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One thing is that can look at totally different variable on the branch of the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflect \n",
    "\n",
    "* This seems very different, because we are no longer using something that appears very linear \n",
    "\n",
    "* Things here are discrete - so something like gradient descent is very tricky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training decision trees\n",
    "\n",
    "We are still in supervised learning realm.  So think about the hypothesis function, the loss function, and minimize the loss.\n",
    "\n",
    "1. Hypothesis function \n",
    "    * This is our tree\n",
    "    \n",
    "2. Parameters \n",
    "    * This is the set of all variables splitting on and the thresholds you split at.\n",
    "    * So a vector that describes the form of the tree\n",
    "    \n",
    "3. The loss function \n",
    "    * Think of y as being binary valued in {0, 1}\n",
    "    * Think of our loss function as being like a bernoulli random variable \n",
    "    * So p(y| h(x)) = So how likely is this to take on the value 1 or not.  \n",
    "\n",
    "Then the loss function: \n",
    "\n",
    "* The log probability of the observed value given our hypothesis.  So the negative log likelihood. \n",
    "    * $ l(h_\\theta )= p(y| h(x)) $\n",
    "    * $ h_\\theta^y(1 - h_\\theta)^{1 - y} $\n",
    "Then take the log likeihood, again \n",
    "    *$ l(h_\\theta, y) = -y log h_\\theta(x) - (1 - y)log(1 - h_\\theta x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So the standard probabilistic loss for binary random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimize the loss function \n",
    "\n",
    "* So how do we minimize this loss function?\n",
    "* $ minimize \\theta \\sum_{i = 1}^m l(h_\\theta, y_i) $\n",
    "\n",
    "So there is no notion of gradients here.  Because again, this is discretized, we have \n",
    "\n",
    "* Also there is $ 2^{2^n}$ possible decision trees.  So there is huge number of all possible trees of our data.\n",
    "* And remember if we can split on the same tree again and again, then probably cannot even bound it.\n",
    "\n",
    "So instead we use the **greedy approach**.\n",
    "\n",
    "* This means we build our tree to continually minimize this loss.\n",
    "* So consider a particular region, if we want to minimize the loss of a region, what should we predict in that region?\n",
    "\n",
    "    * Well it's the proportion of positive examples in that region over the proportion of negative examples in that region \n",
    "    * So this is just like a coin flip.  In this region, I flip a coin and the heads are my positives and the tails are my negatives, then to minimize the loss I pick the h that is the number of positive examples divided by the number of total examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now picking features to split\n",
    "* You cannot do a diagonal split, because are always looking \n",
    "* So when we split, how much are we reducing the overall loss\n",
    "* **Answer**: And we go from the one overall region minus the two new regions.\n",
    "* This is called the information gain.\n",
    "\n",
    "So decision trees will look on a node by node basis, and for all leaf nodes, they will evaluate the information gain.  The amount to decrease the loss if we split on that feature at that leaf node.  \n",
    "\n",
    "* Then we pick the leaf node with the largest information gain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous features\n",
    "\n",
    "* Now sort the examples by the $x_j$ values, and compute innformation gain at each possible split point\n",
    "\n",
    "So we take all the observations, and sort by a particular feature, and sort them.  And move the split point over.  So will consider all possible break points that can have.  \n",
    "\n",
    "* And can compute the best possible split in n time.  \n",
    "* Also benefit is that no need to do any numerical normalization, just can do sorting, because that is all that matters here.\n",
    "* So can run this without preprocessing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting predictions \n",
    "\n",
    "* Why trees are beloved?\n",
    "    * It's the interpretability.  \n",
    "If we asked why it predicted positive one, then it's pretty easy to say.  \n",
    "    * Eg. Cancer cells, so the reason why we predicted is because the mean number and mean area is greater than these numbers\n",
    "    * Whereas a linear regression model says something like these things times the positive and these others, then this whole thing is greater than zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So decision trees give us a criteria as to why we made these decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability matter?\n",
    "\n",
    "1. I don't need the explanation, just do a better job.\n",
    "2. But out classifiers can make foolish mistakes, and interpretability is how we understand this \n",
    "    * Eg. Diagnosing when to discharge people from the hospital.  So discharge based on their breathing.  And said to dismiss people with asthma, but reason why is because these people were never studied.  Basically, because  Rich Karaona\n",
    "    * Response training and testing set mismatch.  But that is an existing problem, so run an interpretable model, and will see these problems.   \n",
    "    * So they are good for debugging and diagnosing problems with ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting with Early stopping and pruning  \n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
