{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Classification is not so different than what we saw in regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. regression, predicting a real value quantity \n",
    "2. But in classification, predicting discrete valued quantity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Binary classification: -1, 1\n",
    "* Multiclass classification: {1, 2, ...k}\n",
    "\n",
    "* Example: Breast cancer clasification.  \n",
    "    The system computes features for each cell, such as area, perimeter concavity, texture for all features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So then compelling that able to extract a process from this.  So this has been an influential study, from being an influence.  So then now a plot of these features.  \n",
    "\n",
    "* Note that for linear regression, it was about drawing a line through the data\n",
    "* For classification, it's about drawing a line that separates the two classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification in ML\n",
    "\n",
    "1. Input features: $x_i \\in R^n i = 1...m$\n",
    "2. Outputs: $y \\in {-1, +1}$\n",
    "3. Parameters: $\\theta \\in R^n$\n",
    "4. Hypothesis function: $h_theta: R^n -> R$\n",
    "    * $h_\\theta(x) = \\sum_{j = 1}^n \\theta_j x_j = \\theta^T x$\n",
    "\n",
    "    * So we assume the hypothesis gives us real value outputs, and the sign tells us if it's positive or negative.  \n",
    "    * So this gives us a way to see how confident we are in our prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So the two axes are showing the dimensions of the input space \n",
    "* So remember the more positive, the more confident we are that it's the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidates for loss functions\n",
    "\n",
    "#### Why not just run least squares?\n",
    "\n",
    "* So why not use regression, with the target being a negative one or positive one.  So can just run least squares on it.\n",
    "\n",
    "* So how do we define a loss function.  And note that least squares is not giving us the correct classifier, even on five simple datapoints.\n",
    "* And the reason why, is because if you use a least squares penalty, we have our outliers that is really positive.  So that point is really positive, but we cannot predict say 5, in that point. \n",
    "\n",
    "* So difference is that we don't care about predicting exactly one for all the positive points, and exactly negative one for all the negative points.  Instead we care about correctly classifying the points by being positive and negative in the right spot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0/1 loss \n",
    "\n",
    "* Why not just count up the number of times that we are right\n",
    "* So could say that add one for each time we are wrong (and perhaps divide by number of occurrences)\n",
    "* So then what we do is the label times the hypothesis is positive or negative \n",
    "    * So if they have the same sign the product will be positive\n",
    "    * If they have different signs then the product is negative.  \n",
    "    * Unfortunately, we cannot use gradient descent on this loss. \n",
    "    * And this becomes a hard problem to minimize because there is no negative slope pointing us in the direction of negative loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viable candidates\n",
    "    1. Logistic loss \n",
    "* $l_{logistic} = log(1 + e^{-y h_\\theta(x)}) $\n",
    "    * As the exponential is larger, you approach a linear line \n",
    "    * So when you use that loss, it's called logistic regression\n",
    "* $l_{hinge} = max 1 - y \\cdot h_theta(x), 0 $\n",
    "    * So this provides a good upper bound on the 0 1 loss\n",
    "* Exponential loss - gets very positive as you go up \n",
    "\n",
    "* Sigmoid function - a smoothe version of the zero one loss.  It gives you the same thing, where you never suffer more than one loss.  Solving the optimization for sigmoid is hard.   \n",
    "#### now consider\n",
    "    * How sensitive are each of these losses to outliers?\n",
    "    * So note, that if you are penalized very heavily for an outlier, the classifier will adjust it's boundary to account for it \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving with gradient descent \n",
    "\n",
    "* Unlike with least squares, where there is a closed form solution.  Here there is not, so we need to solve this with gradient descent.\n",
    "\n",
    "* So gradient descent will be hard to beat.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification\n",
    "\n",
    "* Instead of building one classifier.  Build k different classifiers asking is it class i or not.  \n",
    "* So look at each classifier, and predict the class of whatever one is most confident\n",
    "* So compare with real value outputs for the hypotheses.  And higher will mean more likely.\n",
    "\n",
    "* Or can do soft max loss, aka cross entropy loss\n",
    "    * And it sees if you are putting all of your weight onto one category or splitting it around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
