{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Choosing control variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omitted Variable Bias: Teasing out the effect of a variable \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Omitted variable bias: \n",
    "\n",
    "1. If both schooling and health predict earnings such that\n",
    "\n",
    "    * $ Income = B_0 + BxSchooling + BzHealthStatus + \\epsilon $\n",
    "\n",
    "\n",
    "2. Then a regression that omits health status will overstate the effect of schooling on earnings if:\n",
    "\n",
    "        a. Health status has an independent effect on earnings\n",
    "        b. Schooling and health status are positively correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here's why\n",
    "\n",
    "\n",
    "* $ Income = B_0 + BxSchooling + BzHealthStatus + \\epsilon $\n",
    "\n",
    "\n",
    "* $HealthStatus = C_0 + CxSchooling + z$\n",
    "\n",
    "\n",
    "* $Income = B_0 + B_xSchooling + B_z(C_0 + C_xSchooling)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $Income = B_0 + B_z*C_0 + B_xSchooling +  C_xSchooling +  +$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So remember that point 2 means that as health status goes up, schooling goes up.  So an increase in income that looks like it's associated with increased schooling may really be due to ommitted variable bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example: do firms see economies of scale?\n",
    "\n",
    "* Hypothesis is that larger firms see economies of scale, and therefore costs per revenue would be lower.\n",
    "\n",
    "* Larger size -> More economies of scale -> Lower cost\n",
    "\n",
    "$ CostPerRev  = B_1 *firmSize + \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./resources/cost-size.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So larger size -> Larger Wages -> Higher cost\n",
    "\n",
    "![](./resources/wages-size.png)\n",
    "\n",
    "* $Wage = C0 + C1Size $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ CostPerRev  = -B_1 *firmSize + B_1*Wages(C_1*firmsize + C_0) + \\epsilon $\n",
    "\n",
    "So when we include the wages, the wages pick up the negative effect and larger size gets the rest of it.\n",
    "\n",
    "1. Direct effect - economies of scale\n",
    "2. Indirect effect - the effect of size on wages, and then the effect of wages on cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So control for ommitted variable bias:\n",
    "\n",
    "![](./resources/cost-wages-size.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to deal...\n",
    "\n",
    "Yes, omitted variable bias plagues every regression.  But omitted variable bias is manageble because: \n",
    "\n",
    "1. OVB only significant if the omitted variables are correlated with included variables and are important in their own right.\n",
    "2. Perhaps can determine the direction of the bias.\n",
    "3. Thinking about OVB good for modeling.\n",
    "\n",
    "Also: \n",
    "\n",
    "1. Regress with and without variables both variables.  Is there a big difference? \n",
    "2. Use one as the independent variable and the other as dependent.  Is there a relationship?  What's the p value?\n",
    "3. Model the relationship to explain it.\n",
    "\n",
    "So then can determine if want to tease out just one fo the effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem with junk variables\n",
    "\n",
    "> Junk variable - included without theoretical relevance\n",
    "\n",
    "Problems are that junk variables \n",
    "\n",
    "1. Can be statistically significant via random chance.  If 10 junk variables, then a 40% chance one significant via random chance. \n",
    "2. A junk variable correlated with another valid predictor may also have a strong correlation with the target variable.  So will make a valid predictor appear insignificant.\n",
    "\n",
    "\n",
    "### Multicollinearity\n",
    "\n",
    "$ Y = B_xX + B_zZ $\n",
    "\n",
    "Now X, Y, and Z all move together.  So X or Z affects Y, or maybe both do.  But cannot be sure what is more important.  \n",
    "\n",
    "* If X and Z are highly correlated, then the experiments are not independent.  So difficult to determine which is causing the associated movements in Y.  \n",
    "\n",
    "* This leads to computer reporting large standard errors.  So then can get a high R^2 without significant predictors.  \n",
    "\n",
    "But if enough \"independent\" datapoints, then likely do not have collinearity.\n",
    "> Consider a model with 1000 observations in which predictor variables X and Z have a correlation of .90. Roughly speaking, X and Z move  together 90 percent of the time and move independently 100 times. These 100 “independent experiments” could be enough for your computer to determine with some precision the effects of each predictor on Y. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The amount to choose\n",
    "\n",
    "1. No more than one predictor for every five every observations, if predictors significant \n",
    "2. Not more than one predictor for every 10 if lots of junk variables \n",
    "3. Treat each included category as half a predictor\n",
    "\n",
    "### How to choose \n",
    "\n",
    "1. Trial and error is fine \n",
    "    * Kitchen sink regressions can reduce regression precision and give misleading results \n",
    "\n",
    "The following approach balances concerns about kitchen sink modeling and omitted\n",
    "variable bias. You could do worse than to follow these steps:\n",
    "\n",
    "1) Always begin with a “core” set of predictors that have theoretical relevance, as well as any predictors whose effects you are specifically interested in. You may estimate a “quick and dirty” OLS model at this time.\n",
    "\n",
    "2) Finalize model specification issues\n",
    "\n",
    "3) Add additional predictors that you think might be relevant. You can add them one at a time or one “category” at a time (see next section). Check for the robustness of your initial findings.\n",
    "\n",
    "4) When adding predictors, you should keep all the original predictors in the model, even if they were not significant. Remember, omitted variable bias can cause significant predictors to appear to be insignificant. By adding more variables, your key predictors may become significant.\n",
    "\n",
    "5) At this point, you should know your robust findings. That is the main goal of your research.\n",
    "\n",
    "6) If you insist on producing a “final model”, then you should remove those additional predictors that were not remotely significant.\n",
    "\n",
    "7) You can also remove core predictors if they remain insignificant and you need degrees of freedom. If you are not taxed for degrees of freedom, you may want to keep your core variables, if only to paint the entire picture for your audience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
